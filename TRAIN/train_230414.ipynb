{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "pd.set_option('mode.chained_assignment',  None)\n",
    "class Processing:\n",
    "    def __init__(self, x, test): # 이후 테스트 데이터도 넣는 버전 만들어야함\n",
    "        self.x = x\n",
    "        self.test = test\n",
    "    \n",
    "    def to_time(self, data, where = None):\n",
    "        if where == None:\n",
    "            print('where 넣어야함')\n",
    "            return None\n",
    "        \n",
    "        def to_time(time_list):\n",
    "            if where == 'first':\n",
    "                Time = pd.Series(time_list).astype(str).str.zfill(4)\n",
    "                Time = Time.replace('2400','0000')\n",
    "                return [datetime.strptime(i, '%H%M').strftime(\"%H:%M\") if i != '0nan' else np.NaN for i in Time]\n",
    "            \n",
    "            elif where == 'twice':\n",
    "                Time = pd.Series(time_list)\n",
    "                Time = Time.replace('2400','0000')\n",
    "                # Time = Time.replace('9999','1557')\n",
    "                return [datetime.strptime(i, '%H%M').strftime(\"%H:%M\") for i in Time]  \n",
    "        \n",
    "        def time_gb(x):\n",
    "            if x < 600:\n",
    "                return '0000-0600'\n",
    "            elif x >= 600 and x <= 659:\n",
    "                return '0600-0659'\n",
    "            elif x>=1400 and x<=1459:\n",
    "                return '1400-1459'\n",
    "            elif x>=1200 and x<=1259:\n",
    "                return '1200-1259'\n",
    "            elif x>=1500 and x<=1559:\n",
    "                return '1500-1559'\n",
    "            elif x>=1900 and x<=1959:\n",
    "                return '1900-1959'\n",
    "            elif x>=900 and x<=959:\n",
    "                return '0900-0959'\n",
    "            elif x>=1000 and x<=1059:\n",
    "                return  '1000-1059'\n",
    "            elif x>=2000 and x<=2059:\n",
    "                return '2000-2059'\n",
    "            elif x>=1300 and x<=1359:\n",
    "                return '1300-1359'\n",
    "            elif x>=1100 and x<=1159:\n",
    "                return '1100-1159'\n",
    "            elif x>=800 and x<=859:\n",
    "                return '0800-0859'\n",
    "            elif x>=2200 and x<=2259:\n",
    "                return '2200-2259'\n",
    "            elif x>=1600 and x<=1659:\n",
    "                return '1600-1659'\n",
    "            elif x>=1700 and x<=1759:\n",
    "                return '1700-1759'\n",
    "            elif x>=2100 and x<=2159:\n",
    "                return '2100-2159'\n",
    "            elif x>=700 and x<=759:\n",
    "                return '0700-0759'\n",
    "            elif x>=1800 and x<=1859:\n",
    "                return '1800-1859'\n",
    "            elif x>=1 and x<=559:\n",
    "                return '0001-0559'\n",
    "            elif x>=2300 and x<=2400:\n",
    "                return '2300-2400'\n",
    "            \n",
    "        def distance_gb(x):\n",
    "            if x < 700 :\n",
    "                return 'short_route'\n",
    "            elif x>=700 and x<3000:\n",
    "                return 'mid_route'\n",
    "            elif x>=3000:\n",
    "                return 'long_route'    \n",
    "        \n",
    "        if where == 'first':\n",
    "            time_list = [i if str(i) == 'nan' else str(int(i))  for i in data['Estimated_Departure_Time'] ]\n",
    "            time_list1 = [i if str(i) == 'nan' else str(int(i))  for i in data['Estimated_Arrival_Time'] ]\n",
    "            \n",
    "        elif where == 'twice':\n",
    "            time_list = [i for i in data['Estimated_Departure_Time']]\n",
    "            time_list1 = [i for i in data['Estimated_Arrival_Time']]\n",
    "        data['Estimated_Departure_Time_HH:MM'] = to_time(time_list)\n",
    "        data['Estimated_Arrival_Time_HH:MM'] = to_time(time_list1)\n",
    "    \n",
    "        \n",
    "        # 예상 비행시간 만들기 (분으로 만들기)\n",
    "        data_est_time = []\n",
    "        for i,j in zip(data['Estimated_Arrival_Time_HH:MM'], data['Estimated_Departure_Time_HH:MM']):\n",
    "            if str(i) != 'nan' and str(j) != 'nan':\n",
    "                if (datetime.strptime(str(i), \"%H:%M\") - datetime.strptime(str(j), \"%H:%M\")).total_seconds()/60 >= 0:\n",
    "                    data_est_time.append((datetime.strptime(str(i), \"%H:%M\") - datetime.strptime(str(j), \"%H:%M\")).total_seconds()/60)\n",
    "                else:\n",
    "                    time = datetime.strptime(str(i), \"%H:%M\") - datetime.strptime(str(j), \"%H:%M\") + datetime.strptime('23:59', \"%H:%M\") + timedelta(minutes=1)\n",
    "                    data_est_time.append(timedelta(hours=time.hour,minutes=time.minute ).total_seconds()/60)\n",
    "            else:\n",
    "                data_est_time.append(np.NaN)\n",
    "        \n",
    "        if where == 'first':\n",
    "            data['Estimated_Time'] = data_est_time\n",
    "            data['Dep_time_gb'] = data['Estimated_Departure_Time'].dropna().apply(time_gb)\n",
    "            data['Arr_time_gb'] = data['Estimated_Arrival_Time'].dropna().apply(time_gb)\n",
    "            data['route_gb'] = data['Distance'].apply(distance_gb)\n",
    "            \n",
    "        elif where == 'twice':\n",
    "            data['Estimated_Time'] = data_est_time\n",
    "            data['Dep_time_gb'] = data['Estimated_Departure_Time'].astype(int).apply(time_gb)\n",
    "            data['Arr_time_gb'] = data['Estimated_Arrival_Time'].astype(int).apply(time_gb)\n",
    "            data['route_gb'] = data['Distance'].apply(distance_gb)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    \n",
    "    def fill_airline_and_id(self, data):\n",
    "        \n",
    "        for airline in data['Airline'].dropna().unique():\n",
    "            id = data.loc[data['Airline'] == airline, 'Carrier_ID(DOT)'].dropna().unique()\n",
    "            data.loc[data['Airline'] == airline, 'Carrier_ID(DOT)'] = id[0]\n",
    "                \n",
    "\n",
    "        for id in data['Carrier_ID(DOT)'].dropna().unique():\n",
    "            airline = data.loc[data['Carrier_ID(DOT)'] == id, 'Airline'].dropna().unique()\n",
    "            data.loc[data['Carrier_ID(DOT)'] == id, 'Airline'] = airline[0]\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def fill_state(self, data):\n",
    "        for id in data['Origin_Airport_ID'].dropna().unique():\n",
    "            # 테스트셋에 알수없는 Origin State -> dummy로 채움\n",
    "            try:\n",
    "                data.loc[data['Origin_Airport_ID'] == id, 'Origin_State'] = data.loc[data['Origin_Airport_ID'] == id, 'Origin_State'].dropna().unique()[0]\n",
    "            except:\n",
    "                data.loc[data['Origin_Airport_ID'] == id, 'Origin_State'] = 'dummy'\n",
    "        \n",
    "        for id in data['Destination_Airport_ID'].dropna().unique():\n",
    "            try:\n",
    "                data.loc[data['Destination_Airport_ID'] == id, 'Destination_State'] = data.loc[data['Destination_Airport_ID'] == id, 'Destination_State'].dropna().unique()[0]\n",
    "            except: # 기록이 하나밖에 없음. Youngstown (YNG 공항)\n",
    "                data.loc[data['Destination_Airport_ID'] == id, 'Destination_State'] = 'Youngstown'\n",
    "        return data\n",
    "        \n",
    "    def fill_timedata(self, data, where=None):\n",
    "        if where == 'at':\n",
    "            time1 = 'Estimated_Departure_Time'\n",
    "            time2 = 'Estimated_Arrival_Time'\n",
    "            \n",
    "        elif where == 'dt':\n",
    "            time1 = 'Estimated_Arrival_Time'\n",
    "            time2 = 'Estimated_Departure_Time'\n",
    "            \n",
    "        elif where == 'both':\n",
    "            b_data = data[~((data['Estimated_Arrival_Time'].isna()) & (data['Estimated_Departure_Time'].isna()))]\n",
    "            b_data = b_data.groupby(['Origin_Airport', 'Destination_Airport'])['Estimated_Arrival_Time', 'Estimated_Departure_Time'].value_counts()\n",
    "            b_data = b_data.reset_index()\n",
    "            b_data = b_data.rename(columns={0:'count'})\n",
    "            b_data = b_data.sort_values(['Origin_Airport', 'Destination_Airport', 'count'], ascending=False)\n",
    "            b_data = b_data.drop_duplicates(subset=['Origin_Airport', 'Destination_Airport'], keep='first')\n",
    "\n",
    "            b_data = b_data.set_index(['Origin_Airport', 'Destination_Airport']).drop('count', axis=1)\n",
    "\n",
    "            b = data[(data['Estimated_Arrival_Time'].isna()) & (data['Estimated_Departure_Time'].isna())]\n",
    "            b = b.set_index(['Origin_Airport', 'Destination_Airport'])\n",
    "\n",
    "            b[['Estimated_Arrival_Time', 'Estimated_Departure_Time']] = b_data[['Estimated_Arrival_Time', 'Estimated_Departure_Time']]\n",
    "            \n",
    "            return b.reset_index()\n",
    "        \n",
    "        else:\n",
    "            print('Where 값을 채워주세요. at: 도착시간 보간, dt: 출발시간 보간')\n",
    "            return None\n",
    "            \n",
    "            \n",
    "        \n",
    "        # 같은 출발/도착시간이 있을 경우 가장 많은 출발/도착시간으로 채움\n",
    "        merge_data = data.groupby(['Origin_Airport', 'Destination_Airport', 'Airline'])[time2, time1, 'Estimated_Time'].value_counts()\n",
    "        merge_data = merge_data.reset_index()\n",
    "        merge_data = merge_data.rename(columns={0:'count'})\n",
    "        merge_data = merge_data[~((merge_data[time1].isna()) & (merge_data[time2].isna()))]\n",
    "        merge_data = merge_data.sort_values(['Origin_Airport', 'Destination_Airport', 'count'], ascending=False)\n",
    "        merge_data = merge_data.drop_duplicates(subset=['Origin_Airport', 'Destination_Airport', time1], keep='first')\n",
    "        mt = data.loc[(data[time2].isna()) & ~(data[time1].isna()), ['Origin_Airport', 'Destination_Airport',time2, time1]]\n",
    "        mt = mt.drop(time2, axis=1)\n",
    "        mt = pd.merge(mt, merge_data, how='left', on =['Origin_Airport', 'Destination_Airport', time1])\n",
    "\n",
    "        # 같은 출발/도착시간이 없을 경우 출도착공항 Estimated_Time이 가장 많은 시간을 채우고 출발/도착시간을 계산\n",
    "        et = merge_data.groupby(['Origin_Airport', 'Destination_Airport', 'Airline'])[['Estimated_Time']].value_counts()\n",
    "        et = et.reset_index()\n",
    "        et = et.rename(columns={0:'count'})\n",
    "        et = et.sort_values(['Origin_Airport', 'Destination_Airport', 'Airline', 'count'], ascending=False)\n",
    "        et = et.drop_duplicates(subset=['Origin_Airport', 'Destination_Airport', 'Airline'], keep='first').drop(['count'], axis=1)\n",
    "        mt2 = mt[mt[time2].isna()].drop(['Estimated_Time', 'count'], axis=1)\n",
    "        mt2 = pd.merge(mt2, et, how='left', on =['Origin_Airport', 'Destination_Airport', 'Airline'])\n",
    "\n",
    "        # # mt2-mt 합침\n",
    "        mt= mt.set_index(['Origin_Airport', 'Destination_Airport', 'Airline'])\n",
    "        mt2= mt2.reset_index().set_index(['Origin_Airport', 'Destination_Airport', 'Airline'])\n",
    "        mt.loc[mt['Estimated_Time'].isna(), 'Estimated_Time'] = mt2['Estimated_Time']\n",
    "\n",
    "        # # mt-data 합침 (따로 분리하여 나중에 한번에 채움. 보간한 값을 보간할 때 사용하지 않기 위함)\n",
    "        mt = mt.drop('count', axis=1).reset_index().set_index(['Origin_Airport', 'Destination_Airport', time1])\n",
    "        fill_data = data[~(data[time1].isna()) & (data[time2].isna())]\n",
    "        fill_data = fill_data.set_index(['Origin_Airport', 'Destination_Airport', time1])\n",
    "        fill_data[[time2, 'Estimated_Time']] = mt[[time2, 'Estimated_Time']]\n",
    "        fill_data = fill_data.reset_index()\n",
    "    \n",
    "        return fill_data\n",
    "    \n",
    "    # 시간 채우는 함수\n",
    "    def fill_time(self, data, where = None):\n",
    "        at_data = self.fill_timedata(data, where='at')\n",
    "        dt_data = self.fill_timedata(data, where='dt')\n",
    "        b_data = self.fill_timedata(data, where='both')\n",
    "\n",
    "        data = data.set_index('ID')\n",
    "        at_data = at_data.set_index('ID')\n",
    "        dt_data = dt_data.set_index('ID')\n",
    "        b_data = b_data.set_index('ID')\n",
    "        \n",
    "        data.loc[(data['Estimated_Arrival_Time'].isna()) & ~(data['Estimated_Departure_Time'].isna()), ['Estimated_Arrival_Time', 'Estimated_Time']] = at_data[['Estimated_Arrival_Time', 'Estimated_Time']]\n",
    "        data.loc[(data['Estimated_Departure_Time'].isna())  & ~(data['Estimated_Arrival_Time'].isna()), ['Estimated_Departure_Time', 'Estimated_Time']] = dt_data[['Estimated_Departure_Time', 'Estimated_Time']]\n",
    "        data.loc[(data['Estimated_Departure_Time'].isna()) & (data['Estimated_Arrival_Time'].isna()), ['Estimated_Departure_Time', 'Estimated_Arrival_Time', 'Estimated_Time']] = b_data[['Estimated_Departure_Time', 'Estimated_Arrival_Time', 'Estimated_Time']]\n",
    "        data['Estimated_Departure_Time'] = data['Estimated_Departure_Time'].fillna(9999).astype(int).astype(str)\n",
    "        data['Estimated_Departure_Time'] = data['Estimated_Departure_Time'].str.zfill(4)\n",
    "        data['Estimated_Arrival_Time'] = data['Estimated_Arrival_Time'].fillna(9999).astype(int).astype(str)\n",
    "        data['Estimated_Arrival_Time'] = data['Estimated_Arrival_Time'].str.zfill(4)\n",
    "        \n",
    "        # apply용 함수\n",
    "        def arr_time(x):\n",
    "            try:\n",
    "                x['Estimated_Departure_Time'] = x['Estimated_Departure_Time'].replace('2400','0000')\n",
    "                time = datetime.strptime(x['Estimated_Departure_Time'], \"%H%M\") + timedelta(minutes=x['Estimated_Time'])\n",
    "                hour = str(time.hour)\n",
    "                minute = str(time.minute)\n",
    "                x['Estimated_Arrival_Time'] = hour+minute        \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return x\n",
    "\n",
    "        def dep_time(x):\n",
    "            try:\n",
    "                x['Estimated_Arrival_Time'] = x['Estimated_Arrival_Time'].replace('2400','0000')\n",
    "                time = datetime.strptime(x['Estimated_Arrival_Time'], \"%H%M\") + timedelta(minutes=x['Estimated_Time'])\n",
    "                hour = str(time.hour)\n",
    "                minute = str(time.minute)\n",
    "                x['Estimated_Departure_Time'] = hour+minute        \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        add_time = data.loc[(data['Estimated_Arrival_Time'] == '9999') & (data['Estimated_Departure_Time'] != '9999'), ['Estimated_Departure_Time', 'Estimated_Time', 'Estimated_Arrival_Time']]\n",
    "        t = add_time.apply(arr_time, axis=1)\n",
    "        t= t[~t['Estimated_Time'].isna()]\n",
    "        data.loc[t.index, 'Estimated_Arrival_Time'] = t['Estimated_Arrival_Time']\n",
    "        \n",
    "        add_time = data.loc[(data['Estimated_Arrival_Time'] != '9999') & (data['Estimated_Departure_Time'] == '9999'), ['Estimated_Arrival_Time', 'Estimated_Time', 'Estimated_Departure_Time']]\n",
    "        t = add_time.apply(dep_time, axis=1)\n",
    "        t= t[~t['Estimated_Time'].isna()]\n",
    "        data.loc[t.index, 'Estimated_Departure_Time'] = t['Estimated_Departure_Time']\n",
    "        \n",
    "        if where == 'test':\n",
    "            test = data[~data['Estimated_Time'].isna()]\n",
    "            test = data[data['Estimated_Time'] != 'nan']\n",
    "            test = test[test['Estimated_Arrival_Time'] != '9999']\n",
    "            test = test[test['Estimated_Departure_Time'] != '9999']\n",
    "            \n",
    "            return test, data\n",
    "            \n",
    "\n",
    "        data = data[~data['Estimated_Time'].isna()]\n",
    "        data = data[data['Estimated_Time'] != 'nan']\n",
    "\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def train_processing(self):\n",
    "        data = self.fill_airline_and_id(self.x)\n",
    "        data = self.fill_state(data)\n",
    "        data = self.to_time(data, where='first')\n",
    "        data = self.fill_time(data)\n",
    "        data = self.to_time(data, where='twice')\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def test_processing(self):\n",
    "        data = self.fill_airline_and_id(self.test)\n",
    "        data = self.fill_state(data)\n",
    "        data = self.to_time(data, where='first')\n",
    "        data, origin = self.fill_time(data, where='test')\n",
    "        data = self.to_time(data, where='twice')\n",
    "        # return data\n",
    "    \n",
    "        return data, origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\signlab026\\AppData\\Local\\Temp\\ipykernel_12300\\2017646892.py:176: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  merge_data = data.groupby(['Origin_Airport', 'Destination_Airport', 'Airline'])[time2, time1, 'Estimated_Time'].value_counts()\n",
      "C:\\Users\\signlab026\\AppData\\Local\\Temp\\ipykernel_12300\\2017646892.py:176: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  merge_data = data.groupby(['Origin_Airport', 'Destination_Airport', 'Airline'])[time2, time1, 'Estimated_Time'].value_counts()\n",
      "C:\\Users\\signlab026\\AppData\\Local\\Temp\\ipykernel_12300\\2017646892.py:154: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  b_data = b_data.groupby(['Origin_Airport', 'Destination_Airport'])['Estimated_Arrival_Time', 'Estimated_Departure_Time'].value_counts()\n",
      "C:\\Users\\signlab026\\AppData\\Local\\Temp\\ipykernel_12300\\2017646892.py:176: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  merge_data = data.groupby(['Origin_Airport', 'Destination_Airport', 'Airline'])[time2, time1, 'Estimated_Time'].value_counts()\n",
      "C:\\Users\\signlab026\\AppData\\Local\\Temp\\ipykernel_12300\\2017646892.py:176: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  merge_data = data.groupby(['Origin_Airport', 'Destination_Airport', 'Airline'])[time2, time1, 'Estimated_Time'].value_counts()\n",
      "C:\\Users\\signlab026\\AppData\\Local\\Temp\\ipykernel_12300\\2017646892.py:154: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  b_data = b_data.groupby(['Origin_Airport', 'Destination_Airport'])['Estimated_Arrival_Time', 'Estimated_Departure_Time'].value_counts()\n"
     ]
    }
   ],
   "source": [
    "pc = Processing(data, test)\n",
    "\n",
    "train = pc.train_processing()\n",
    "test, origin = pc.test_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE(x):\n",
    "    x = x.drop(['Carrier_Code(IATA)', 'Cancelled', 'Diverted', 'Estimated_Departure_Time', 'Estimated_Arrival_Time'], axis=1)\n",
    "    x['edt_h'] = x['Estimated_Departure_Time_HH:MM'].apply(lambda x:int(x.split(':')[0]))\n",
    "    x['edt_m'] = x['Estimated_Departure_Time_HH:MM'].apply(lambda x:int(x.split(':')[1]))\n",
    "    x['eat_h'] = x['Estimated_Arrival_Time_HH:MM'].apply(lambda x:int(x.split(':')[0]))\n",
    "    x['eat_m'] = x['Estimated_Arrival_Time_HH:MM'].apply(lambda x:int(x.split(':')[1]))\n",
    "\n",
    "    x = x.drop(['Estimated_Departure_Time_HH:MM', 'Estimated_Arrival_Time_HH:MM'], axis=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "train = FE(train)\n",
    "test = FE(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month                          0\n",
      "Day_of_Month                   0\n",
      "Origin_Airport                 0\n",
      "Origin_Airport_ID              0\n",
      "Origin_State                   0\n",
      "Destination_Airport            0\n",
      "Destination_Airport_ID         0\n",
      "Destination_State              0\n",
      "Distance                       0\n",
      "Airline                    11391\n",
      "Carrier_ID(DOT)            11391\n",
      "Tail_Number                    0\n",
      "Delay                     715743\n",
      "Estimated_Time                 0\n",
      "Dep_time_gb                    0\n",
      "Arr_time_gb                    0\n",
      "route_gb                       0\n",
      "edt_h                          0\n",
      "edt_m                          0\n",
      "eat_h                          0\n",
      "eat_m                          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isna().sum())\n",
    "train[['Airline', 'Carrier_ID(DOT)']] = train[['Airline', 'Carrier_ID(DOT)']].fillna('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month                         0\n",
      "Day_of_Month                  0\n",
      "Origin_Airport                0\n",
      "Origin_Airport_ID             0\n",
      "Origin_State                  0\n",
      "Destination_Airport           0\n",
      "Destination_Airport_ID        0\n",
      "Destination_State             0\n",
      "Distance                      0\n",
      "Airline                   10416\n",
      "Carrier_ID(DOT)           10416\n",
      "Tail_Number                   0\n",
      "Estimated_Time                0\n",
      "Dep_time_gb                   0\n",
      "Arr_time_gb                   0\n",
      "route_gb                      0\n",
      "edt_h                         0\n",
      "edt_m                         0\n",
      "eat_h                         0\n",
      "eat_m                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test.isna().sum())\n",
    "test[['Airline', 'Carrier_ID(DOT)']] = test[['Airline', 'Carrier_ID(DOT)']].fillna('dummy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month                          0\n",
      "Day_of_Month                   0\n",
      "Origin_Airport                 0\n",
      "Origin_Airport_ID              0\n",
      "Origin_State                   0\n",
      "Destination_Airport            0\n",
      "Destination_Airport_ID         0\n",
      "Destination_State              0\n",
      "Distance                       0\n",
      "Airline                        0\n",
      "Carrier_ID(DOT)                0\n",
      "Tail_Number                    0\n",
      "Delay                     715743\n",
      "Estimated_Time                 0\n",
      "Dep_time_gb                    0\n",
      "Arr_time_gb                    0\n",
      "route_gb                       0\n",
      "edt_h                          0\n",
      "edt_m                          0\n",
      "eat_h                          0\n",
      "eat_m                          0\n",
      "dtype: int64\n",
      "Month                     0\n",
      "Day_of_Month              0\n",
      "Origin_Airport            0\n",
      "Origin_Airport_ID         0\n",
      "Origin_State              0\n",
      "Destination_Airport       0\n",
      "Destination_Airport_ID    0\n",
      "Destination_State         0\n",
      "Distance                  0\n",
      "Airline                   0\n",
      "Carrier_ID(DOT)           0\n",
      "Tail_Number               0\n",
      "Estimated_Time            0\n",
      "Dep_time_gb               0\n",
      "Arr_time_gb               0\n",
      "route_gb                  0\n",
      "edt_h                     0\n",
      "edt_m                     0\n",
      "eat_h                     0\n",
      "eat_m                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isna().sum())\n",
    "\n",
    "print(test.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Carrier_ID(DOT)'] = train['Carrier_ID(DOT)'].astype(str)\n",
    "test['Carrier_ID(DOT)'] = test['Carrier_ID(DOT)'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_ar = train.__deepcopy__().drop(['Destination_Airport_ID', 'Origin_Airport_ID'], axis=1)\n",
    "test_ar = test.__deepcopy__().drop(['Destination_Airport_ID', 'Origin_Airport_ID'], axis=1)\n",
    "\n",
    "train_ar['Carrier_ID(DOT)'] = train_ar['Carrier_ID(DOT)'].astype(str)\n",
    "test_ar['Carrier_ID(DOT)'] = test_ar['Carrier_ID(DOT)'].astype(str)\n",
    "\n",
    "label_st = train_ar.drop(columns = ['Delay'])\n",
    "train_st = train_ar[~(train_ar['Delay'].isna()) & (train_ar['route_gb'] != 'long_route')]\n",
    "test_st = test_ar[test_ar['route_gb'] != 'long_route']\n",
    "X_nan = train_ar[(train_ar['Delay'].isna()) & (train_ar['route_gb'] != 'long_route')]\n",
    "\n",
    "qual_col = ['Origin_Airport', 'Origin_State', 'Destination_Airport', 'Destination_State', 'Airline', 'Carrier_ID(DOT)', 'Tail_Number', 'Dep_time_gb',\t'Arr_time_gb',\t'route_gb']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le=le.fit(train[i])\n",
    "    train_st[i]=le.transform(train_st[i])\n",
    "    X_nan[i]=le.transform(X_nan[i])\n",
    "    \n",
    "    for label in np.unique(test[i]):\n",
    "        if label not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test_st[i]=le.transform(test_st[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon.core as ag\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_data = TabularDataset(train_st)\n",
    "unlabeled_data = TabularDataset(X_nan)\n",
    "test_data = TabularDataset(test_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230414_073037\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=3, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230414_073037\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19044\n",
      "Train Data Rows:    244468\n",
      "Train Data Columns: 18\n",
      "Label Column: Delay\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = Not_Delayed, class 0 = Delayed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (Not_Delayed) vs negative (Delayed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5082.4 MB\n",
      "\tTrain Data (Original)  Memory Usage: 99.74 MB (2.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  2 | ['Distance', 'Estimated_Time']\n",
      "\t\t('int', [])   : 16 | ['Month', 'Day_of_Month', 'Origin_Airport', 'Origin_State', 'Destination_Airport', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  2 | ['Distance', 'Estimated_Time']\n",
      "\t\t('int', [])       : 15 | ['Month', 'Day_of_Month', 'Origin_Airport', 'Origin_State', 'Destination_Airport', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['route_gb']\n",
      "\t1.2s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 96.87 MB (1.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1199.21s of the 3598.54s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-1.3275\t = Validation score   (-log_loss)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t35.26s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1162.44s of the 3561.77s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-1.3345\t = Validation score   (-log_loss)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t38.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1123.85s of the 3523.18s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4394\t = Validation score   (-log_loss)\n",
      "\t61.17s\t = Training   runtime\n",
      "\t21.94s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1045.31s of the 3444.64s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4378\t = Validation score   (-log_loss)\n",
      "\t26.39s\t = Training   runtime\n",
      "\t7.59s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1010.22s of the 3409.55s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 251 due to low memory. Expected memory usage reduced from 17.86% -> 15.0% of available memory...\n",
      "\t-0.4495\t = Validation score   (-log_loss)\n",
      "\t29.09s\t = Training   runtime\n",
      "\t7.0s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 973.28s of the 3372.61s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 250 due to low memory. Expected memory usage reduced from 17.96% -> 15.0% of available memory...\n",
      "\t-0.4494\t = Validation score   (-log_loss)\n",
      "\t31.03s\t = Training   runtime\n",
      "\t6.56s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 934.96s of the 3334.29s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4376\t = Validation score   (-log_loss)\n",
      "\t281.07s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 647.22s of the 3046.54s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 274 due to low memory. Expected memory usage reduced from 16.38% -> 15.0% of available memory...\n",
      "\t-0.4478\t = Validation score   (-log_loss)\n",
      "\t18.49s\t = Training   runtime\n",
      "\t6.56s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 621.27s of the 3020.6s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 276 due to low memory. Expected memory usage reduced from 16.27% -> 15.0% of available memory...\n",
      "\t-0.4474\t = Validation score   (-log_loss)\n",
      "\t18.92s\t = Training   runtime\n",
      "\t6.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 595.11s of the 2994.44s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4458\t = Validation score   (-log_loss)\n",
      "\t242.33s\t = Training   runtime\n",
      "\t2.55s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 345.58s of the 2744.91s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4379\t = Validation score   (-log_loss)\n",
      "\t28.22s\t = Training   runtime\n",
      "\t2.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 307.45s of the 2706.78s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4483\t = Validation score   (-log_loss)\n",
      "\t247.09s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 52.86s of the 2452.19s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4373\t = Validation score   (-log_loss)\n",
      "\t31.16s\t = Training   runtime\n",
      "\t10.31s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2411.97s of remaining time.\n",
      "\t-0.4365\t = Validation score   (-log_loss)\n",
      "\t13.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1065.47s of the 1954.32s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4362\t = Validation score   (-log_loss)\n",
      "\t22.49s\t = Training   runtime\n",
      "\t2.83s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1035.94s of the 1924.79s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4363\t = Validation score   (-log_loss)\n",
      "\t19.1s\t = Training   runtime\n",
      "\t1.46s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1010.26s of the 1899.12s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4383\t = Validation score   (-log_loss)\n",
      "\t76.77s\t = Training   runtime\n",
      "\t9.42s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 923.4s of the 1812.26s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4381\t = Validation score   (-log_loss)\n",
      "\t97.43s\t = Training   runtime\n",
      "\t9.57s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 815.69s of the 1704.54s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4362\t = Validation score   (-log_loss)\n",
      "\t94.14s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 714.67s of the 1603.53s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4355\t = Validation score   (-log_loss)\n",
      "\t26.68s\t = Training   runtime\n",
      "\t7.88s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 679.44s of the 1568.3s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4357\t = Validation score   (-log_loss)\n",
      "\t27.35s\t = Training   runtime\n",
      "\t7.87s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 643.53s of the 1532.38s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4363\t = Validation score   (-log_loss)\n",
      "\t305.19s\t = Training   runtime\n",
      "\t2.43s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 330.65s of the 1219.51s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4365\t = Validation score   (-log_loss)\n",
      "\t20.28s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 303.62s of the 1192.47s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4369\t = Validation score   (-log_loss)\n",
      "\t214.13s\t = Training   runtime\n",
      "\t1.52s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 82.66s of the 971.52s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4363\t = Validation score   (-log_loss)\n",
      "\t27.95s\t = Training   runtime\n",
      "\t3.44s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 936.71s of remaining time.\n",
      "\t-0.4345\t = Validation score   (-log_loss)\n",
      "\t10.92s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 11 L3 models ...\n",
      "Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 617.0s of the 359.7s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4348\t = Validation score   (-log_loss)\n",
      "\t13.37s\t = Training   runtime\n",
      "\t1.81s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L3 ... Training model for up to 594.07s of the 336.79s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4346\t = Validation score   (-log_loss)\n",
      "\t9.86s\t = Training   runtime\n",
      "\t1.18s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L3 ... Training model for up to 576.5s of the 319.22s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4401\t = Validation score   (-log_loss)\n",
      "\t79.94s\t = Training   runtime\n",
      "\t8.8s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L3 ... Training model for up to 487.04s of the 229.76s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4395\t = Validation score   (-log_loss)\n",
      "\t108.18s\t = Training   runtime\n",
      "\t9.86s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L3 ... Training model for up to 368.34s of the 111.05s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4347\t = Validation score   (-log_loss)\n",
      "\t68.74s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L3 ... Training model for up to 292.55s of the 35.26s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4364\t = Validation score   (-log_loss)\n",
      "\t27.39s\t = Training   runtime\n",
      "\t8.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L3 ... Training model for up to 256.37s of the -0.92s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\t-0.4363\t = Validation score   (-log_loss)\n",
      "\t26.76s\t = Training   runtime\n",
      "\t7.96s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L3 ... Training model for up to 220.91s of the -36.37s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4343\t = Validation score   (-log_loss)\n",
      "\t158.25s\t = Training   runtime\n",
      "\t2.56s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L3 ... Training model for up to 55.03s of the -202.26s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4349\t = Validation score   (-log_loss)\n",
      "\t14.19s\t = Training   runtime\n",
      "\t1.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 31.56s of the -225.73s of remaining time.\n",
      "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-0.4354\t = Validation score   (-log_loss)\n",
      "\t25.81s\t = Training   runtime\n",
      "\t1.8s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.0s of the -260.48s of remaining time.\n",
      "\t-0.4343\t = Validation score   (-log_loss)\n",
      "\t10.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 11 L4 models ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 5...\n",
      "AutoGluon training complete, total runtime = 4563.81s ... Best model: \"WeightedEnsemble_L4\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230414_073037\\\")\n"
     ]
    }
   ],
   "source": [
    "label = 'Delay'\n",
    "eval_metric = 'log_loss'\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=label, problem_type='binary', eval_metric=eval_metric\n",
    ").fit(train_data=train_st,\n",
    "      unlabeled_data = X_nan.drop('Delay', axis=1), \n",
    "      presets='best_quality', \n",
    "      num_stack_levels=3,\n",
    "      #excluded_model_types = excluded_model_types,\n",
    "      time_limit=3600, num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model  score_val  pred_time_val     fit_time  \\\n",
      "0       WeightedEnsemble_L4  -0.434253     206.340862  2166.794833   \n",
      "1    NeuralNetFastAI_BAG_L3  -0.434304     196.017325  2105.197898   \n",
      "2       WeightedEnsemble_L3  -0.434519     184.167486  1282.527040   \n",
      "3           LightGBM_BAG_L3  -0.434620     194.632312  1956.805536   \n",
      "4           CatBoost_BAG_L3  -0.434706     193.533270  2015.693641   \n",
      "5         LightGBMXT_BAG_L3  -0.434785     195.266387  1960.321317   \n",
      "6            XGBoost_BAG_L3  -0.434871     194.632829  1961.137229   \n",
      "7     NeuralNetTorch_BAG_L3  -0.435441     195.255331  1972.759447   \n",
      "8     ExtraTreesGini_BAG_L2  -0.435463     153.846262  1042.118782   \n",
      "9     ExtraTreesEntr_BAG_L2  -0.435663     153.840167  1042.784746   \n",
      "10          CatBoost_BAG_L2  -0.436185     146.044016  1109.579495   \n",
      "11        LightGBMXT_BAG_L2  -0.436203     148.795568  1037.931556   \n",
      "12     LightGBMLarge_BAG_L2  -0.436274     149.407193  1043.388942   \n",
      "13          LightGBM_BAG_L2  -0.436282     147.421584  1034.540028   \n",
      "14    ExtraTreesEntr_BAG_L3  -0.436292     201.417175  1973.713342   \n",
      "15   NeuralNetFastAI_BAG_L2  -0.436339     148.391093  1320.629400   \n",
      "16    ExtraTreesGini_BAG_L3  -0.436415     201.453377  1974.342599   \n",
      "17           XGBoost_BAG_L2  -0.436492     146.962559  1035.720498   \n",
      "18      WeightedEnsemble_L2  -0.436509      20.409839   380.823288   \n",
      "19    NeuralNetTorch_BAG_L2  -0.436867     147.489093  1229.564321   \n",
      "20     LightGBMLarge_BAG_L1  -0.437307      10.311970    31.156321   \n",
      "21          CatBoost_BAG_L1  -0.437586       0.118942   281.071791   \n",
      "22          LightGBM_BAG_L1  -0.437844       7.594964    26.386226   \n",
      "23           XGBoost_BAG_L1  -0.437872       2.365991    28.223921   \n",
      "24  RandomForestEntr_BAG_L2  -0.438087     155.535125  1112.863297   \n",
      "25  RandomForestGini_BAG_L2  -0.438271     155.384771  1092.204496   \n",
      "26        LightGBMXT_BAG_L1  -0.439412      21.943187    61.169631   \n",
      "27  RandomForestEntr_BAG_L3  -0.439478     203.314202  2055.127499   \n",
      "28  RandomForestGini_BAG_L3  -0.440093     202.257488  2026.891253   \n",
      "29   NeuralNetFastAI_BAG_L1  -0.445762       2.551037   242.330827   \n",
      "30    ExtraTreesEntr_BAG_L1  -0.447396       6.451913    18.919799   \n",
      "31    ExtraTreesGini_BAG_L1  -0.447801       6.562707    18.491113   \n",
      "32    NeuralNetTorch_BAG_L1  -0.448253       1.095433   247.090308   \n",
      "33  RandomForestEntr_BAG_L1  -0.449379       6.561393    31.033106   \n",
      "34  RandomForestGini_BAG_L1  -0.449532       7.000527    29.088436   \n",
      "35    KNeighborsUnif_BAG_L1  -1.327514      35.262865     0.231158   \n",
      "36    KNeighborsDist_BAG_L1  -1.334493      38.145086     0.245021   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                 0.013024          10.787784            4       True   \n",
      "1                 2.560057         158.248913            3       True   \n",
      "2                 0.018032          10.917408            3       True   \n",
      "3                 1.175045           9.856550            3       True   \n",
      "4                 0.076002          68.744655            3       True   \n",
      "5                 1.809119          13.372332            3       True   \n",
      "6                 1.175561          14.188243            3       True   \n",
      "7                 1.798063          25.810461            3       True   \n",
      "8                 7.880246          26.681125            2       True   \n",
      "9                 7.874151          27.347088            2       True   \n",
      "10                0.077999          94.141838            2       True   \n",
      "11                2.829551          22.493899            2       True   \n",
      "12                3.441177          27.951284            2       True   \n",
      "13                1.455567          19.102371            2       True   \n",
      "14                7.959908          26.764357            3       True   \n",
      "15                2.425077         305.191743            2       True   \n",
      "16                7.996109          27.393613            3       True   \n",
      "17                0.996542          20.282840            2       True   \n",
      "18                0.017971          13.985029            2       True   \n",
      "19                1.523076         214.126663            2       True   \n",
      "20               10.311970          31.156321            1       True   \n",
      "21                0.118942         281.071791            1       True   \n",
      "22                7.594964          26.386226            1       True   \n",
      "23                2.365991          28.223921            1       True   \n",
      "24                9.569109          97.425639            2       True   \n",
      "25                9.418755          76.766838            2       True   \n",
      "26               21.943187          61.169631            1       True   \n",
      "27                9.856935         108.178514            3       True   \n",
      "28                8.800220          79.942267            3       True   \n",
      "29                2.551037         242.330827            1       True   \n",
      "30                6.451913          18.919799            1       True   \n",
      "31                6.562707          18.491113            1       True   \n",
      "32                1.095433         247.090308            1       True   \n",
      "33                6.561393          31.033106            1       True   \n",
      "34                7.000527          29.088436            1       True   \n",
      "35               35.262865           0.231158            1       True   \n",
      "36               38.145086           0.245021            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          37  \n",
      "1          34  \n",
      "2          26  \n",
      "3          28  \n",
      "4          31  \n",
      "5          27  \n",
      "6          35  \n",
      "7          36  \n",
      "8          20  \n",
      "9          21  \n",
      "10         19  \n",
      "11         15  \n",
      "12         25  \n",
      "13         16  \n",
      "14         33  \n",
      "15         22  \n",
      "16         32  \n",
      "17         23  \n",
      "18         14  \n",
      "19         24  \n",
      "20         13  \n",
      "21          7  \n",
      "22          4  \n",
      "23         11  \n",
      "24         18  \n",
      "25         17  \n",
      "26          3  \n",
      "27         30  \n",
      "28         29  \n",
      "29         10  \n",
      "30          9  \n",
      "31          8  \n",
      "32         12  \n",
      "33          6  \n",
      "34          5  \n",
      "35          1  \n",
      "36          2  \n"
     ]
    }
   ],
   "source": [
    "print(predictor.leaderboard(silent = True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = predictor.get_model_best()\n",
    "model_pred = predictor.predict_proba(test_st, model=model_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.fit_pseudolabel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6ad6a5e467af0c55cd3439f95cdffc9b087666e3ab0338166755486d36b79e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
